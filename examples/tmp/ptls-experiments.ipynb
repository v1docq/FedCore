{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "common file to explore the compatibility \n",
       "\n",
       "of composite compression framework and ptls models"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
       "import os\n",
       "\n",
       "os.chdir('..')"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
       "import torch\n",
       "import torch.nn\n",
       "import torchvision.datasets\n",
       "from fedot.core.pipelines.pipeline_builder import PipelineBuilder\n",
       "from torch import optim, nn\n",
       "from torch.utils.data import DataLoader, Dataset\n",
       "from torchvision.models import resnet18\n",
       "from torchvision.transforms import transforms\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "from fedcore.tools.ruler import PerformanceEvaluator\n",
       "\n",
       "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
       "from tqdm import tqdm\n",
       "import json\n",
       "from datetime import datetime\n",
       "def save_json(obj, path):\n",
       "    with open(path, 'w') as file:\n",
       "        json.dump(obj, file)\n",
       "\n",
       "def load_json(path):\n",
       "    with open(path, 'r') as file:\n",
       "        return json.load(file)\n",
       "    \n",
       "import pickle\n",
       "\n",
       "def save_pkl(obj, path):\n",
       "    with open(path, 'wb') as file:\n",
       "        pickle.dump(obj, file)\n",
       "\n",
       "def load_pkl(path):\n",
       "    with open(path, 'rb') as file:\n",
       "        return pickle.load(file)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/html": [
          "<div>\n",
          "<style scoped>\n",
          "    .dataframe tbody tr th:only-of-type {\n",
          "        vertical-align: middle;\n",
          "    }\n",
          "\n",
          "    .dataframe tbody tr th {\n",
          "        vertical-align: top;\n",
          "    }\n",
          "\n",
          "    .dataframe thead th {\n",
          "        text-align: right;\n",
          "    }\n",
          "</style>\n",
          "<table border=\"1\" class=\"dataframe\">\n",
          "  <thead>\n",
          "    <tr style=\"text-align: right;\">\n",
          "      <th></th>\n",
          "      <th>client_id</th>\n",
          "      <th>trans_date</th>\n",
          "      <th>small_group</th>\n",
          "      <th>amount_rur</th>\n",
          "    </tr>\n",
          "  </thead>\n",
          "  <tbody>\n",
          "    <tr>\n",
          "      <th>0</th>\n",
          "      <td>33172</td>\n",
          "      <td>6</td>\n",
          "      <td>4</td>\n",
          "      <td>71.463</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>1</th>\n",
          "      <td>33172</td>\n",
          "      <td>6</td>\n",
          "      <td>35</td>\n",
          "      <td>45.017</td>\n",
          "    </tr>\n",
          "  </tbody>\n",
          "</table>\n",
          "</div>"
         ],
         "text/plain": [
          "   client_id  trans_date  small_group  amount_rur\n",
          "0      33172           6            4      71.463\n",
          "1      33172           6           35      45.017"
         ]
        },
        "execution_count": 5,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "import os\n",
       "import pandas as pd\n",
       "\n",
       "# source_data = pd.read_csv('https://huggingface.co/datasets/dllllb/age-group-prediction/resolve/main/transactions_train.csv.gz?download=true', \n",
       "#                           compression='gzip',\n",
       "#                           nrows=450_577)\n",
       "source_data = pd.read_csv('/ptls-experiments/scenario_age_pred/notebooks/data/transactions_train.csv',\n",
       "                          nrows=1_450_577)\n",
       "source_data.head(2)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
       "from ptls.preprocessing import PandasDataPreprocessor\n",
       "import pickle\n",
       "\n",
       "if not os.path.exists('preprocessor_age-group-prediction.p'):\n",
       "    preprocessor = PandasDataPreprocessor(\n",
       "        col_id='client_id',\n",
       "        col_event_time='trans_date',\n",
       "        event_time_transformation='none',\n",
       "        cols_category=['small_group'],\n",
       "        cols_numerical=['amount_rur'],\n",
       "        return_records=True,\n",
       "    )\n",
       "    preprocessor.fit(source_data)\n",
       "    with open('preprocessor_age-group-prediction.p', 'wb') as file:\n",
       "        pickle.dump(preprocessor, file)\n",
       "else:\n",
       "    with open('preprocessor_age-group-prediction.p', 'rb') as file:\n",
       "        preprocessor = pickle.load(file) "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
       "dataset = preprocessor.transform(source_data)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "(1332, 333)"
         ]
        },
        "execution_count": 8,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "dataset = sorted(dataset, key=lambda x: x['client_id'])\n",
       "from sklearn.model_selection import train_test_split\n",
       "\n",
       "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
       "\n",
       "len(train), len(test)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
       "from functools import partial\n",
       "from ptls.nn import TrxEncoder, RnnSeqEncoder, GptEncoder\n",
       "from ptls.nn.head import Head\n",
       "from ptls.frames.coles import CoLESModule\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
       "def estimate_size(model):\n",
       "    param_size = torch.zeros((1,), dtype=torch.float64)\n",
       "    for param in model.parameters():\n",
       "        param_size += param.nelement() * param.element_size()\n",
       "    buffer_size = torch.zeros_like(param_size)\n",
       "    for buffer in model.buffers():\n",
       "        buffer_size += buffer.nelement() * buffer.element_size()\n",
       "\n",
       "    size_all_mb = param_size / (1 << 20) + buffer_size / (1 << 20)\n",
       "    print('model size: {:.3f}MB'.format(size_all_mb.item()))\n",
       "    return size_all_mb.item()\n",
       "\n",
       "\n",
       "# estimate_size(model)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
       "def predict_ptls(model, predict_dataloader):\n",
       "    def get_device(model):\n",
       "        return next(iter(model.parameters()))\n",
       "    device = get_device(model)\n",
       "    model.eval()\n",
       "    predictions = []\n",
       "    with torch.no_grad():\n",
       "        for batch in predict_dataloader:\n",
       "            if isinstance(batch, tuple):\n",
       "                X = batch[0]\n",
       "            else:\n",
       "                X = batch\n",
       "\n",
       "            predictions.append(\n",
       "                model(X.to(device))\n",
       "            )\n",
       "    return torch.cat(predictions)\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "2024-09-29 15:43:34,357 - Loaded 1332 records\n",
         "2024-09-29 15:43:34,360 - Loaded 333 records\n"
        ]
       }
      ],
      "source": [
       "from ptls.data_load.datasets import MemoryMapDataset\n",
       "from ptls.data_load.iterable_processing import SeqLenFilter\n",
       "from ptls.frames.coles import ColesDataset\n",
       "from ptls.frames.coles.split_strategy import SampleSlices\n",
       "from ptls.frames import PtlsDataModule\n",
       "\n",
       "ptls_data_module = PtlsDataModule(\n",
       "    train_data=ColesDataset(\n",
       "        MemoryMapDataset(\n",
       "            data=train,\n",
       "            i_filters=[\n",
       "                SeqLenFilter(min_seq_len=25),\n",
       "            ],\n",
       "        ),\n",
       "        splitter=SampleSlices(\n",
       "            split_count=5,\n",
       "            cnt_min=25,\n",
       "            cnt_max=200,\n",
       "        ),\n",
       "    ),\n",
       "    train_num_workers=4,\n",
       "    train_batch_size=256,\n",
       "    test_data=ColesDataset(\n",
       "        MemoryMapDataset(\n",
       "            data=test,\n",
       "            i_filters=[\n",
       "                SeqLenFilter(min_seq_len=25),\n",
       "            ],\n",
       "        ),\n",
       "        splitter=SampleSlices(\n",
       "            split_count=5,\n",
       "            cnt_min=25,\n",
       "            cnt_max=200,\n",
       "        ),\n",
       "    ),\n",
       "    test_num_workers=4,\n",
       "    test_batch_size=256,\n",
       ")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
       "from ptls.frames.coles.losses.contrastive_loss import ContrastiveLoss\n",
       "from ptls.frames.coles.sampling_strategies.hard_negative_pair_selector import HardNegativePairSelector\n",
       "\n",
       "\n",
       "from fedcore.api.utils.data import get_compression_input\n",
       "\n",
       "# input_data = get_compression_input(model, \n",
       "#                                    ptls_data_module.train_dataloader(), \n",
       "#                                    ptls_data_module.test_dataloader(),\n",
       "#                                    num_classes=1,\n",
       "#                                    train_loss=partial(ContrastiveLoss, margin=0.5, sampling_strategy=HardNegativePairSelector(neg_count=5))\n",
       "#                                 )\n",
       "\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
       "from fedcore.api.main import FedCore\n",
       "from fedcore.api.utils.evaluation import evaluate_original_model, evaluate_optimised_model\n",
       "\n",
       "experiment_setup = {'compression_task': 'training',\n",
       "                    # 'cv_task': 'classification',\n",
       "                    'model_params': dict(epochs=1,\n",
       "                                         )\n",
       "}\n",
       "# experiment_setup = {'compression_task': 'pruning',\n",
       "#                     # 'cv_task': 'classification',\n",
       "#                     'model_params': dict(epochs=1,\n",
       "#                                          pruning_iterations=1,\n",
       "#                                          learning_rate=0.001,\n",
       "#                                          importance='MagnitudeImportance',\n",
       "#                                          pruner_name='magnitude_pruner',\n",
       "#                                          importance_norm=1,\n",
       "#                                          pruning_ratio=0.5,\n",
       "#                                          finetune_params={'epochs': 1,\n",
       "#                                                           'custom_loss': None,\n",
       "#                                                         #   {'ptls_contrastive': CONTRASTIVE_LOSS()}\n",
       "#                                                           }\n",
       "#                                          )\n",
       "# }\n",
       "# experiment_setup = {'compression_task': 'low_rank',\n",
       "#                     # 'cv_task': 'classification',\n",
       "#                     'model_params': dict(epochs=1,\n",
       "#                                          learning_rate=0.001,\n",
       "#                                          hoyer_loss=0.2,\n",
       "#                                          energy_thresholds=[0.99],\n",
       "#                                          orthogonal_loss=5,\n",
       "#                                          decomposing_mode='channel',\n",
       "#                                          spectrum_pruning_strategy='energy',\n",
       "#                                          finetune_params={'epochs': 1,\n",
       "#                                                           'custom_loss': None}\n",
       "#                                          )\n",
       "#                     }\n",
       "\n",
       "# experiment_setup = {'compression_task': 'composite_compression',\n",
       "#                     'model_params': dict(pruning_model=dict(epochs=1,\n",
       "#                                                             pruning_iterations=3,\n",
       "#                                                             learning_rate=0.001,\n",
       "#                                                             importance='MagnitudeImportance',\n",
       "#                                                             pruner_name='magnitude_pruner',\n",
       "#                                                             importance_norm=1,\n",
       "#                                                             pruning_ratio=0.75,\n",
       "#                                                             finetune_params={'epochs': 5,\n",
       "#                                                                              'custom_loss': None}\n",
       "#                                                             ),\n",
       "#                                          low_rank_model=dict(epochs=5,\n",
       "#                                                              learning_rate=0.001,\n",
       "#                                                              hoyer_loss=0.2,\n",
       "#                                                              energy_thresholds=[0.9],\n",
       "#                                                              orthogonal_loss=5,\n",
       "#                                                              decomposing_mode='channel',\n",
       "#                                                              spectrum_pruning_strategy='energy',\n",
       "#                                                              finetune_params={'epochs': 10,\n",
       "#                                                                               'custom_loss': None}\n",
       "#                                                              ),\n",
       "#                                          training_model=dict(\n",
       "#                                              epochs=1,\n",
       "#                                          )                    \n",
       "#                                          ),\n",
       "                                         \n",
       "#                     'initial_assumption': ['training_model',\n",
       "#                                             'pruning_model',\n",
       "#                                               'low_rank_model'\n",
       "#                                               ]}\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# DATA_DIR = '../scenario_age_pred/notebooks/data'\n",
       "# def run_experiment_(DATA_DIR, loss, define_model, fedcore_setup, save_dir, n_cls=2, folds=range(5)):\n",
       "#     log_path = os.path.join(save_dir, 'exp.json')\n",
       "\n",
       "#     res = []\n",
       "#     for fold_i in folds:\n",
       "#         exp_res = {'fold': fold_i} \n",
       "\n",
       "#         df_trx_pretrain = pd.read_pickle(f'{DATA_DIR}/fold_{fold_i}/df_trx_pretrain.pickle')\n",
       "#         df_seq_pretrain = pd.read_pickle(f'{DATA_DIR}/fold_{fold_i}/df_seq_pretrain.pickle')\n",
       "\n",
       "#         with open(f'data/fold_{fold_i}/pdp.pickle', 'rb') as f:\n",
       "#             pdp = pickle.load(f)\n",
       "            \n",
       "#         df_seq_pretrain_train, df_seq_pretrain_valid = train_test_split(\n",
       "#             df_seq_pretrain, test_size=0.05, shuffle=True, random_state=42)\n",
       "        \n",
       "#         coles_data_module = ptls.frames.PtlsDataModule(\n",
       "#         train_data=ptls.frames.coles.ColesDataset(\n",
       "#             data=ptls.data_load.datasets.MemoryMapDataset(\n",
       "#                 df_seq_pretrain_train.to_dict(orient='records') + \n",
       "#                 df_trx_pretrain.to_dict(orient='records')\n",
       "#             ),\n",
       "#             splitter=ptls.frames.coles.split_strategy.SampleSlices(\n",
       "#                 split_count=5,\n",
       "#                 cnt_min=25,\n",
       "#                 cnt_max=200,\n",
       "#             ),\n",
       "#         ),\n",
       "#         valid_data=ptls.frames.coles.ColesDataset(\n",
       "#             data=ptls.data_load.datasets.MemoryMapDataset(\n",
       "#                 df_seq_pretrain_train.to_dict(orient='records')),\n",
       "#             splitter=ptls.frames.coles.split_strategy.SampleSlices(\n",
       "#                 split_count=5,\n",
       "#                 cnt_min=25,\n",
       "#                 cnt_max=100,\n",
       "#             ),\n",
       "#         ),\n",
       "#         train_batch_size=64,\n",
       "#         train_num_workers=4,\n",
       "#         valid_batch_size=650,\n",
       "#         )\n",
       "\n",
       "#         model = define_model()\n",
       "#         input_data  = get_compression_input(model, \n",
       "#                                     coles_data_module.train_dataloader(), \n",
       "#                                     coles_data_module.valid_dataloader(),\n",
       "#                                     num_classes=n_cls,\n",
       "#                                     train_loss=loss\n",
       "#                                     )\n",
       "#         fedcore_compressor = FedCore(**fedcore_setup)\n",
       "#         training_time_0 = datetime.now().timestamp()\n",
       "#         exp_res['training_time_0'] = training_time_0\n",
       "#         fedcore_compressor.fit((input_data, model), manually_done=True)\n",
       "#         training_time_1 = datetime.now().timestamp()\n",
       "#         exp_res['training_time_1'] = training_time_1\n",
       "\n",
       "#         save_json(res, log_path)\n",
       "\n",
       "#         del input_data\n",
       "#         del model\n",
       "#     return fedcore_compressor\n",
       "    \n",
       "    "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
       "def run_experiment(directory, ptls_data_module, loss, define_model, fedcore_setup, n_cls=2, n=3):\n",
       "    log_path = os.path.join(directory, 'exp.json')\n",
       "    \n",
       "    res = []\n",
       "    for i in tqdm(range(n)):\n",
       "        exp_res = {'iter': i} \n",
       "        model = define_model()\n",
       "        input_data  = get_compression_input(model, \n",
       "                                   ptls_data_module.train_dataloader(), \n",
       "                                   ptls_data_module.test_dataloader(),\n",
       "                                   num_classes=n_cls,\n",
       "                                   train_loss=loss\n",
       "                                )\n",
       "\n",
       "        start_size = estimate_size(model)\n",
       "        exp_res['start_size'] = start_size\n",
       "        \n",
       "        \n",
       "        fedcore_compressor = FedCore(**fedcore_setup)\n",
       "        training_time_0 = datetime.now().timestamp()\n",
       "        exp_res['training_time_0'] = training_time_0\n",
       "\n",
       "        fedcore_compressor.fit((input_data, model), manually_done=True)\n",
       "\n",
       "        training_time_1 = datetime.now().timestamp()\n",
       "        exp_res['training_time_1'] = training_time_1\n",
       "\n",
       "        end_size = estimate_size(fedcore_compressor.optimised_model)\n",
       "        exp_res['end_size'] = end_size\n",
       "        exp_res['orig_size'] = estimate_size(fedcore_compressor.original_model)\n",
       "        save_pkl(fedcore_compressor.optimised_model, path=os.path.join(directory, f'model_{i}.pkl'))\n",
       "        save_pkl(fedcore_compressor.original_model, path=os.path.join(directory, f'model_{i}_or.pkl'))\n",
       "        res.append(exp_res)\n",
       "        save_json(res, log_path)\n",
       "\n",
       "        del input_data\n",
       "        del model\n",
       "    return fedcore_compressor"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
       "def define_coles():\n",
       "    trx_encoder_params = dict(\n",
       "    embeddings_noise=0.003,\n",
       "    numeric_values={'amount_rur': 'identity'},\n",
       "    embeddings={\n",
       "        'trans_date': {'in': 800, 'out': 16},\n",
       "        'small_group': {'in': 250, 'out': 16},\n",
       "    },\n",
       ")\n",
       "\n",
       "    seq_encoder = RnnSeqEncoder(\n",
       "        trx_encoder=TrxEncoder(**trx_encoder_params),\n",
       "        hidden_size=256,\n",
       "        type='gru',\n",
       "    )\n",
       "\n",
       "    model = CoLESModule(\n",
       "        seq_encoder=seq_encoder,\n",
       "        optimizer_partial=partial(torch.optim.Adam, lr=0.001),\n",
       "        lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=30, gamma=0.9),\n",
       "        head=Head(use_norm_encoder=True, \n",
       "                input_size=256,\n",
       "                hidden_layers_sizes=[256, 256])\n",
       "    )\n",
       "    return model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "fedcore_compressor = run_experiment('compression_experiments/composite_age', \n",
       "               ptls_data_module,\n",
       "               partial(ContrastiveLoss, margin=0.5, sampling_strategy=HardNegativePairSelector(neg_count=5)),\n",
       "               define_coles,\n",
       "               fedcore_setup = {'compression_task': 'composite_compression',\n",
       "                    'common': dict(save_each=5),\n",
       "                    'model_params': dict(pruning_model=dict(epochs=2,\n",
       "                                                            pruning_iterations=3,\n",
       "                                                            learning_rate=0.001,\n",
       "                                                            importance='MagnitudeImportance',\n",
       "                                                            pruner_name='magnitude_pruner',\n",
       "                                                            importance_norm=1,\n",
       "                                                            pruning_ratio=0.75,\n",
       "                                                            finetune_params={'epochs': 1,\n",
       "                                                                             'custom_loss': None}\n",
       "                                                            ),\n",
       "                                         low_rank_model=dict(epochs=30,\n",
       "                                                             learning_rate=0.001,\n",
       "                                                             hoyer_loss=0.2,\n",
       "                                                             energy_thresholds=[0.9],\n",
       "                                                             orthogonal_loss=5,\n",
       "                                                             decomposing_mode='channel',\n",
       "                                                             spectrum_pruning_strategy='energy',\n",
       "                                                             finetune_params={'epochs': 1,\n",
       "                                                                              'custom_loss': None}\n",
       "                                                             ),\n",
       "                                         training_model=dict(\n",
       "                                             epochs=10,\n",
       "                                         )                    \n",
       "                                         ),  \n",
       "                    'initial_assumption': [\n",
       "                        # 'training_model',\n",
       "                                            # 'pruning_model', \n",
       "                                            'low_rank_model',\n",
       "                                            #   'pruning_model'\n",
       "                                              ]},\n",
       "              n =1\n",
       "               )"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "CoLESModule(\n",
          "  (_loss): ContrastiveLoss()\n",
          "  (_seq_encoder): RnnSeqEncoder(\n",
          "    (trx_encoder): TrxEncoder(\n",
          "      (embeddings): ModuleDict(\n",
          "        (trans_date): NoisyEmbedding(\n",
          "          800, 16, padding_idx=0\n",
          "          (dropout): Dropout(p=0, inplace=False)\n",
          "        )\n",
          "        (small_group): NoisyEmbedding(\n",
          "          250, 16, padding_idx=0\n",
          "          (dropout): Dropout(p=0, inplace=False)\n",
          "        )\n",
          "      )\n",
          "      (custom_embeddings): ModuleDict(\n",
          "        (amount_rur): IdentityScaler()\n",
          "      )\n",
          "      (custom_embedding_batch_norm): RBatchNorm(\n",
          "        (bn): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
          "      )\n",
          "    )\n",
          "    (seq_encoder): RnnEncoder(\n",
          "      (rnn): GRU(33, 256, batch_first=True)\n",
          "      (reducer): LastStepEncoder()\n",
          "    )\n",
          "  )\n",
          "  (_validation_metric): BatchRecallTopK()\n",
          "  (_head): Head(\n",
          "    (model): Sequential(\n",
          "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
          "      (1): ReLU()\n",
          "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
          "      (3): ReLU()\n",
          "      (4): L2NormEncoder()\n",
          "    )\n",
          "  )\n",
          ")"
         ]
        },
        "execution_count": 31,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "fedcore_compressor.original_model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "model size: 1.420MB\n",
         "model size: 1.423MB\n"
        ]
       },
       {
        "data": {
         "text/plain": [
          "(1.4195785522460938, 1.4226303100585938)"
         ]
        },
        "execution_count": 32,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "estimate_size(fedcore_compressor.original_model), estimate_size(fedcore_compressor.optimised_model)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# NSP"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "2024-09-23 14:51:20,543 - Loaded 1332 records\n",
         "2024-09-23 14:51:20,546 - Loaded 333 records\n"
        ]
       }
      ],
      "source": [
       "from ptls.data_load.datasets import MemoryMapDataset\n",
       "from ptls.data_load.iterable_processing import SeqLenFilter\n",
       "from ptls.frames.bert import NspDataset\n",
       "from ptls.frames.coles.split_strategy import SampleSlices\n",
       "from ptls.frames import PtlsDataModule\n",
       "\n",
       "nsp_dl = PtlsDataModule(\n",
       "    train_data=NspDataset(\n",
       "        MemoryMapDataset(\n",
       "            data=train,\n",
       "            i_filters=[\n",
       "                SeqLenFilter(min_seq_len=25),\n",
       "            ],\n",
       "        ),\n",
       "        splitter=SampleSlices(\n",
       "            split_count=5,\n",
       "            cnt_min=25,\n",
       "            cnt_max=200,\n",
       "        ),\n",
       "    ),\n",
       "    train_num_workers=16,\n",
       "    train_batch_size=256,\n",
       "    test_data=NspDataset(\n",
       "        MemoryMapDataset(\n",
       "            data=test,\n",
       "            i_filters=[\n",
       "                SeqLenFilter(min_seq_len=25),\n",
       "            ],\n",
       "        ),\n",
       "        splitter=SampleSlices(\n",
       "            split_count=5,\n",
       "            cnt_min=25,\n",
       "            cnt_max=200,\n",
       "        ),\n",
       "    ),\n",
       "    test_num_workers=16,\n",
       "    test_batch_size=256,\n",
       ")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "torch.nn.modules.transformer.TransformerEncoderLayer"
         ]
        },
        "execution_count": 35,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "nn.TransformerEncoderLayer"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
       "from functools import partial\n",
       "from ptls.nn import TrxEncoder, RnnSeqEncoder\n",
       "from ptls.frames.bert import SopNspModule\n",
       "\n",
       "def define_nsp():\n",
       "    trx_encoder_params = dict(\n",
       "        embeddings_noise=0.003,\n",
       "        numeric_values={'amount_rur': 'identity'},\n",
       "        embeddings={\n",
       "            'trans_date': {'in': 800, 'out': 16},\n",
       "            'small_group': {'in': 250, 'out': 16},\n",
       "        },\n",
       "    )\n",
       "\n",
       "    seq_encoder = RnnSeqEncoder(\n",
       "        trx_encoder=TrxEncoder(**trx_encoder_params),\n",
       "        hidden_size=32,\n",
       "        type='lstm',\n",
       "    )\n",
       "\n",
       "    model = SopNspModule(\n",
       "        seq_encoder=seq_encoder,\n",
       "        hidden_size = 256,\n",
       "        drop_p = 0.2,\n",
       "        optimizer_partial=partial(torch.optim.Adam, lr=0.001),\n",
       "        lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=30, gamma=0.9),\n",
       "    )\n",
       "    return model"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "В Batch Handler нужно подавать функцию (batch) -> (X, y), где Х, y: loss(y, model(x))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "class DecomposedParameter(torch.nn.parameter.Parameter):\n",
       "    def __init__(self, data, requires_grad=True):\n",
       "        U, S, Vh = torch.linalg.svd(data, full_matrices=False)\n",
       "        self.U = torch.nn.Parameter(U)\n",
       "        self.S = torch.nn.Parameter(S)\n",
       "        self.Vh = torch.nn.Parameter(Vh)\n",
       "        \n",
       "    \n",
       "        \n",
       "\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "  0%|          | 0/1 [00:00<?, ?it/s]"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "model size: 0.101MB\n",
         "2024-09-23 14:52:10,442 - Initialising experiment setup\n",
         "2024-09-23 14:52:10,672 - Initialising Industrial Repository\n",
         "2024-09-23 14:52:10,676 - Initialising solver\n",
         "2024-09-23 14:52:10,677 - Initialising experiment setup\n",
         "Forcely substituted loss to BCELoss(\n",
         "  (loss): BCELoss()\n",
         ")\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "  0%|          | 0/6 [00:12<?, ?it/s]\n",
         "  0%|          | 0/1 [00:13<?, ?it/s]\n"
        ]
       },
       {
        "ename": "AttributeError",
        "evalue": "'tuple' object has no attribute 'to'",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
         "Cell \u001b[0;32mIn[119], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mptls\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCELoss\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcompression_experiments/composite_age/nsp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m               \u001b[49m\u001b[43mnsp_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m               \u001b[49m\u001b[43mBCELoss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m               \u001b[49m\u001b[43mdefine_nsp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m               \u001b[49m\u001b[43mfedcore_setup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcompression_task\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomposite_compression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_params\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpruning_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mpruning_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mimportance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMagnitudeImportance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mpruner_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmagnitude_pruner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mimportance_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mpruning_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mfinetune_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                                                             \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mlow_rank_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mhoyer_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43menergy_thresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43morthogonal_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mdecomposing_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchannel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mspectrum_pruning_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menergy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mfinetune_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtraining_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43m)\u001b[49m\u001b[43m                    \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                         \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minitial_assumption\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpruning_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlow_rank_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpruning_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m              \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     39\u001b[0m \u001b[43m              \u001b[49m\u001b[43m)\u001b[49m\n",
         "Cell \u001b[0;32mIn[99], line 23\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(directory, ptls_data_module, loss, define_model, fedcore_setup, n_cls, n)\u001b[0m\n\u001b[1;32m     20\u001b[0m training_time_0 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtimestamp()\n\u001b[1;32m     21\u001b[0m exp_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_time_0\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m training_time_0\n\u001b[0;32m---> 23\u001b[0m \u001b[43mfedcore_compressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanually_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m training_time_1 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtimestamp()\n\u001b[1;32m     26\u001b[0m exp_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_time_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m training_time_1\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/api/main.py:153\u001b[0m, in \u001b[0;36mFedCore.fit\u001b[0;34m(self, input_data, manually_done, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data \u001b[38;5;241m=\u001b[39m input_preproc\u001b[38;5;241m.\u001b[39mcheck_input_data(manually_done)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_solver()\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimised_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver\u001b[38;5;241m.\u001b[39mroot_node\u001b[38;5;241m.\u001b[39mfitted_operation\u001b[38;5;241m.\u001b[39moptimised_model\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver\u001b[38;5;241m.\u001b[39mroot_node\u001b[38;5;241m.\u001b[39mfitted_operation\u001b[38;5;241m.\u001b[39mmodel\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/pipeline.py:197\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, input_data, time_constraint, n_jobs)\u001b[0m\n\u001b[1;32m    194\u001b[0m copied_input_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_data_to_nodes(copied_input_data)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     train_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopied_input_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     train_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_with_time_limit(input_data\u001b[38;5;241m=\u001b[39mcopied_input_data, time\u001b[38;5;241m=\u001b[39mtime_constraint)\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/pipeline.py:112\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, input_data, process_state_dict, fitted_operations)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Timer() \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[1;32m    111\u001b[0m     computation_time_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_node\u001b[38;5;241m.\u001b[39mfitted_operation \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputation_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     train_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m computation_time_update:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputation_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(t\u001b[38;5;241m.\u001b[39mminutes_from_start, \u001b[38;5;241m3\u001b[39m)\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:196\u001b[0m, in \u001b[0;36mPipelineNode.fit\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs training process in the node\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    OutputData: values predicted on the provided ``input_data``\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrying to fit pipeline node with operation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m input_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_input_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_operation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Timer() \u001b[38;5;28;01mas\u001b[39;00m t:\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:273\u001b[0m, in \u001b[0;36mPipelineNode._get_input_data\u001b[0;34m(self, input_data, parent_operation)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_input_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data: InputData, parent_operation: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes_from:\n\u001b[0;32m--> 273\u001b[0m         input_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_from_parents\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_operation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_operation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirect_set:\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:299\u001b[0m, in \u001b[0;36mPipelineNode._input_from_parents\u001b[0;34m(self, input_data, parent_operation)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFit all parent nodes in secondary node with operation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    297\u001b[0m parent_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_from_with_fixed_order()\n\u001b[0;32m--> 299\u001b[0m parent_results, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_combine_parents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mparent_operation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m secondary_input \u001b[38;5;241m=\u001b[39m DataMerger\u001b[38;5;241m.\u001b[39mget(parent_results)\u001b[38;5;241m.\u001b[39mmerge()\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Update info about visited nodes\u001b[39;00m\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:390\u001b[0m, in \u001b[0;36m_combine_parents\u001b[0;34m(parent_nodes, input_data, parent_operation)\u001b[0m\n\u001b[1;32m    388\u001b[0m     parent_results\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m parent_operation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 390\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     parent_results\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:196\u001b[0m, in \u001b[0;36mPipelineNode.fit\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs training process in the node\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    OutputData: values predicted on the provided ``input_data``\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrying to fit pipeline node with operation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m input_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_input_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_operation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Timer() \u001b[38;5;28;01mas\u001b[39;00m t:\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:273\u001b[0m, in \u001b[0;36mPipelineNode._get_input_data\u001b[0;34m(self, input_data, parent_operation)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_input_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data: InputData, parent_operation: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes_from:\n\u001b[0;32m--> 273\u001b[0m         input_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_from_parents\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_operation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_operation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirect_set:\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:299\u001b[0m, in \u001b[0;36mPipelineNode._input_from_parents\u001b[0;34m(self, input_data, parent_operation)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFit all parent nodes in secondary node with operation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    297\u001b[0m parent_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_from_with_fixed_order()\n\u001b[0;32m--> 299\u001b[0m parent_results, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_combine_parents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mparent_operation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m secondary_input \u001b[38;5;241m=\u001b[39m DataMerger\u001b[38;5;241m.\u001b[39mget(parent_results)\u001b[38;5;241m.\u001b[39mmerge()\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Update info about visited nodes\u001b[39;00m\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:390\u001b[0m, in \u001b[0;36m_combine_parents\u001b[0;34m(parent_nodes, input_data, parent_operation)\u001b[0m\n\u001b[1;32m    388\u001b[0m     parent_results\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m parent_operation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 390\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     parent_results\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:196\u001b[0m, in \u001b[0;36mPipelineNode.fit\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs training process in the node\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    OutputData: values predicted on the provided ``input_data``\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrying to fit pipeline node with operation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m input_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_input_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_operation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Timer() \u001b[38;5;28;01mas\u001b[39;00m t:\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:273\u001b[0m, in \u001b[0;36mPipelineNode._get_input_data\u001b[0;34m(self, input_data, parent_operation)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_input_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data: InputData, parent_operation: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes_from:\n\u001b[0;32m--> 273\u001b[0m         input_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_from_parents\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_operation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_operation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirect_set:\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:299\u001b[0m, in \u001b[0;36mPipelineNode._input_from_parents\u001b[0;34m(self, input_data, parent_operation)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFit all parent nodes in secondary node with operation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    297\u001b[0m parent_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_from_with_fixed_order()\n\u001b[0;32m--> 299\u001b[0m parent_results, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_combine_parents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mparent_operation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m secondary_input \u001b[38;5;241m=\u001b[39m DataMerger\u001b[38;5;241m.\u001b[39mget(parent_results)\u001b[38;5;241m.\u001b[39mmerge()\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Update info about visited nodes\u001b[39;00m\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:390\u001b[0m, in \u001b[0;36m_combine_parents\u001b[0;34m(parent_nodes, input_data, parent_operation)\u001b[0m\n\u001b[1;32m    388\u001b[0m     parent_results\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m parent_operation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 390\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     parent_results\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:200\u001b[0m, in \u001b[0;36mPipelineNode.fit\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Timer() \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[0;32m--> 200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation, operation_predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m                                                                      \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_time_in_seconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(t\u001b[38;5;241m.\u001b[39mseconds_from_start, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/repository/fedcore_impl/abstract.py:77\u001b[0m, in \u001b[0;36m_fit\u001b[0;34m(self, params, data)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This method is used for defining and running of the evaluation strategy\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03mto train the operation with the data provided\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    tuple: trained operation and prediction on train data\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(data\u001b[38;5;241m.\u001b[39mtask, params\u001b[38;5;241m=\u001b[39mparams, n_samples_data\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m predict_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_for_fit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation, data, params)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation, predict_train\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/interfaces/fedcore_strategy.py:48\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(self, train_data)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation_impl \u001b[38;5;241m=\u001b[39m BaseNeuralModel(params)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# self.operation_impl = self._convert_to_operation(operation_type)(self.params_for_fit)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_data: InputData):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mtarget\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation_impl\u001b[38;5;241m.\u001b[39mfit(train_data)\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/models/network_impl/base_nn_model.py:79\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(self, input_data, supplementary_data)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mtarget\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     81\u001b[0m fit_output \u001b[38;5;241m=\u001b[39m Either(value\u001b[38;5;241m=\u001b[39msupplementary_data,\n\u001b[1;32m     82\u001b[0m                     monoid\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_loss, custom_fit_process])\u001b[38;5;241m.\u001b[39meither(\n\u001b[1;32m     83\u001b[0m     left_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m custom_loss: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_train(loader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, custom_loss),\n\u001b[1;32m     84\u001b[0m     right_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m sup_data: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_train(loader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, sup_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_cache()\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/pymonad/either.py:91\u001b[0m, in \u001b[0;36mEither.either\u001b[0;34m(self, left_function, right_function)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m right_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mleft_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonoid\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/models/network_impl/base_nn_model.py:81\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(custom_loss)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 81\u001b[0m fit_output \u001b[38;5;241m=\u001b[39m Either(value\u001b[38;5;241m=\u001b[39msupplementary_data,\n\u001b[1;32m     82\u001b[0m                     monoid\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_loss, custom_fit_process])\u001b[38;5;241m.\u001b[39meither(\n\u001b[1;32m     83\u001b[0m     left_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m custom_loss: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_train(loader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, custom_loss),\n\u001b[1;32m     84\u001b[0m     right_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m sup_data: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_train(loader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, sup_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_cache()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/models/network_impl/base_nn_model.py:139\u001b[0m, in \u001b[0;36m_default_train\u001b[0;34m(self, train_loader, model, custom_loss)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_train\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m                    train_loader,\n\u001b[1;32m    137\u001b[0m                    model,\n\u001b[1;32m    138\u001b[0m                    custom_loss: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    141\u001b[0m         model_loss, avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_loop(train_loader, model, custom_loss)\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/models/network_impl/base_nn_model.py:97\u001b[0m, in \u001b[0;36m_train_loop\u001b[0;34m(self, train_loader, model, custom_loss)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 97\u001b[0m     total_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     98\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     99\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n",
         "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
        ]
       }
      ],
      "source": [
       "from ptls.loss import BCELoss\n",
       "\n",
       "run_experiment('compression_experiments/composite_age/nsp', \n",
       "               nsp_dl,\n",
       "               BCELoss,\n",
       "               define_nsp,\n",
       "               fedcore_setup = {'compression_task': 'composite_compression',\n",
       "                    'model_params': dict(pruning_model=dict(epochs=5,\n",
       "                                                            pruning_iterations=3,\n",
       "                                                            learning_rate=0.001,\n",
       "                                                            importance='MagnitudeImportance',\n",
       "                                                            pruner_name='magnitude_pruner',\n",
       "                                                            importance_norm=1,\n",
       "                                                            pruning_ratio=0.75,\n",
       "                                                            finetune_params={'epochs': 5,\n",
       "                                                                             'custom_loss': None}\n",
       "                                                            ),\n",
       "                                         low_rank_model=dict(epochs=30,\n",
       "                                                             learning_rate=0.001,\n",
       "                                                             hoyer_loss=0.2,\n",
       "                                                             energy_thresholds=[0.9],\n",
       "                                                             orthogonal_loss=5,\n",
       "                                                             decomposing_mode='channel',\n",
       "                                                             spectrum_pruning_strategy='energy',\n",
       "                                                             finetune_params={'epochs': 10,\n",
       "                                                                              'custom_loss': None}\n",
       "                                                             ),\n",
       "                                         training_model=dict(\n",
       "                                             epochs=100,\n",
       "                                         )                    \n",
       "                                         ),\n",
       "                                         \n",
       "                    'initial_assumption': ['training_model',\n",
       "                                            'pruning_model', \n",
       "                                            'low_rank_model',\n",
       "                                              'pruning_model'\n",
       "                                              ]},\n",
       "              n =1\n",
       "              )"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
       "_ = next(iter(nsp_dl.train_dataloader()))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "((<ptls.data_load.padded_batch.PaddedBatch at 0x7f05fc6c0f70>,\n",
          "  <ptls.data_load.padded_batch.PaddedBatch at 0x7f05b4253880>),\n",
          " tensor([1, 1, 1,  ..., 0, 0, 0]))"
         ]
        },
        "execution_count": 121,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "_"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# MLM"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "2024-09-23 14:34:45,939 - Loaded 1332 records\n",
         "2024-09-23 14:34:45,944 - Loaded 333 records\n"
        ]
       }
      ],
      "source": [
       "import torch\n",
       "from ptls.nn import TrxEncoder, LongformerEncoder\n",
       "from ptls.frames.bert import MLMPretrainModule\n",
       "from ptls.nn import PBLinear, PBL2Norm, PBLayerNorm\n",
       "from ptls.data_load.datasets import MemoryMapDataset\n",
       "from ptls.data_load.iterable_processing import FeatureFilter\n",
       "from ptls.frames.bert import MlmDataset\n",
       "from ptls.frames import PtlsDataModule\n",
       "\n",
       "mlm_dm = PtlsDataModule(\n",
       "    train_data=MlmDataset(\n",
       "        MemoryMapDataset(\n",
       "            data=train,\n",
       "        ),\n",
       "        min_len=100, max_len=128\n",
       "    ),\n",
       "    test_data=MlmDataset(\n",
       "        MemoryMapDataset(\n",
       "            data=test,\n",
       "            i_filters=[\n",
       "                FeatureFilter(),\n",
       "            ],\n",
       "        ),\n",
       "        min_len=200, max_len=256\n",
       "    ),\n",
       "    train_num_workers=16,\n",
       "    train_batch_size=128,\n",
       ")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
       "def define_mlm():\n",
       "    trx_encoder_params = dict(\n",
       "    embeddings_noise=0.003,\n",
       "    numeric_values={'amount_rur': 'identity'},\n",
       "    embeddings={\n",
       "        'trans_date': {'in': 800, 'out': 16},\n",
       "        'small_group': {'in': 250, 'out': 16},\n",
       "    },\n",
       "    )\n",
       "    trx_encoder = TrxEncoder(**trx_encoder_params)\n",
       "\n",
       "    mlm_module = MLMPretrainModule(\n",
       "    trx_encoder=torch.nn.Sequential(\n",
       "        trx_encoder,\n",
       "        PBLinear(trx_encoder.output_size, 64),\n",
       "        PBL2Norm(),\n",
       "    ),\n",
       "    seq_encoder=LongformerEncoder(\n",
       "        input_size=64,\n",
       "        num_attention_heads=1,\n",
       "        intermediate_size=256,\n",
       "        num_hidden_layers=2,\n",
       "        attention_window=32,\n",
       "        max_position_embeddings=2000,\n",
       "    ),\n",
       "    hidden_size=256,\n",
       "    loss_temperature=20.0,\n",
       "    \n",
       "    total_steps=30000,\n",
       "\n",
       "    replace_proba=0.1,\n",
       "    neg_count=64,\n",
       "    \n",
       "    log_logits=True,\n",
       "    )\n",
       "    return mlm_module"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "  0%|          | 0/1 [00:00<?, ?it/s]"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "model size: 1.040MB\n",
         "2024-09-23 14:34:50,883 - Initialising experiment setup\n",
         "2024-09-23 14:34:51,093 - Initialising Industrial Repository\n",
         "2024-09-23 14:34:51,095 - Initialising solver\n",
         "2024-09-23 14:34:51,096 - Initialising experiment setup\n",
         "Forcely substituted loss to QuerySoftmaxLoss()\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "  0%|          | 0/11 [00:13<?, ?it/s]\n",
         "  0%|          | 0/1 [00:13<?, ?it/s]\n"
        ]
       },
       {
        "ename": "TypeError",
        "evalue": "cannot unpack non-iterable PaddedBatch object",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
         "Cell \u001b[0;32mIn[107], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mptls\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_soft_max\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuerySoftmaxLoss\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcompression_experiments/composite_age/mlm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmlm_dm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m               \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQuerySoftmaxLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m               \u001b[49m\u001b[43mdefine_mlm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m               \u001b[49m\u001b[43mfedcore_setup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcompression_task\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomposite_compression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_params\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpruning_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mpruning_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mimportance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMagnitudeImportance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mpruner_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmagnitude_pruner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mimportance_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mpruning_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mfinetune_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                                                             \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mlow_rank_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mhoyer_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43menergy_thresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43morthogonal_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mdecomposing_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchannel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mspectrum_pruning_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menergy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mfinetune_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtraining_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43m)\u001b[49m\u001b[43m                    \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                         \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minitial_assumption\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;66;43;03m# 'pruning_model', \u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;66;43;03m# 'low_rank_model',\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;66;43;03m#   'pruning_model'\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m              \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     39\u001b[0m \u001b[43m               \u001b[49m\u001b[43m)\u001b[49m\n",
         "Cell \u001b[0;32mIn[99], line 23\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(directory, ptls_data_module, loss, define_model, fedcore_setup, n_cls, n)\u001b[0m\n\u001b[1;32m     20\u001b[0m training_time_0 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtimestamp()\n\u001b[1;32m     21\u001b[0m exp_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_time_0\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m training_time_0\n\u001b[0;32m---> 23\u001b[0m \u001b[43mfedcore_compressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanually_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m training_time_1 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtimestamp()\n\u001b[1;32m     26\u001b[0m exp_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_time_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m training_time_1\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/api/main.py:153\u001b[0m, in \u001b[0;36mFedCore.fit\u001b[0;34m(self, input_data, manually_done, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data \u001b[38;5;241m=\u001b[39m input_preproc\u001b[38;5;241m.\u001b[39mcheck_input_data(manually_done)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_solver()\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimised_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver\u001b[38;5;241m.\u001b[39mroot_node\u001b[38;5;241m.\u001b[39mfitted_operation\u001b[38;5;241m.\u001b[39moptimised_model\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver\u001b[38;5;241m.\u001b[39mroot_node\u001b[38;5;241m.\u001b[39mfitted_operation\u001b[38;5;241m.\u001b[39mmodel\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/pipeline.py:197\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, input_data, time_constraint, n_jobs)\u001b[0m\n\u001b[1;32m    194\u001b[0m copied_input_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_data_to_nodes(copied_input_data)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     train_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopied_input_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     train_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_with_time_limit(input_data\u001b[38;5;241m=\u001b[39mcopied_input_data, time\u001b[38;5;241m=\u001b[39mtime_constraint)\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/pipeline.py:112\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, input_data, process_state_dict, fitted_operations)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Timer() \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[1;32m    111\u001b[0m     computation_time_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_node\u001b[38;5;241m.\u001b[39mfitted_operation \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputation_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     train_predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m computation_time_update:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputation_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(t\u001b[38;5;241m.\u001b[39mminutes_from_start, \u001b[38;5;241m3\u001b[39m)\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/fedot/core/pipelines/node.py:200\u001b[0m, in \u001b[0;36mPipelineNode.fit\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Timer() \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[0;32m--> 200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation, operation_predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m                                                                      \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_time_in_seconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(t\u001b[38;5;241m.\u001b[39mseconds_from_start, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/repository/fedcore_impl/abstract.py:77\u001b[0m, in \u001b[0;36m_fit\u001b[0;34m(self, params, data)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This method is used for defining and running of the evaluation strategy\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03mto train the operation with the data provided\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    tuple: trained operation and prediction on train data\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(data\u001b[38;5;241m.\u001b[39mtask, params\u001b[38;5;241m=\u001b[39mparams, n_samples_data\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m predict_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_for_fit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation, data, params)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted_operation, predict_train\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/interfaces/fedcore_strategy.py:48\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(self, train_data)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation_impl \u001b[38;5;241m=\u001b[39m BaseNeuralModel(params)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# self.operation_impl = self._convert_to_operation(operation_type)(self.params_for_fit)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_data: InputData):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mtarget\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation_impl\u001b[38;5;241m.\u001b[39mfit(train_data)\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/models/network_impl/base_nn_model.py:79\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(self, input_data, supplementary_data)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mtarget\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     81\u001b[0m fit_output \u001b[38;5;241m=\u001b[39m Either(value\u001b[38;5;241m=\u001b[39msupplementary_data,\n\u001b[1;32m     82\u001b[0m                     monoid\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_loss, custom_fit_process])\u001b[38;5;241m.\u001b[39meither(\n\u001b[1;32m     83\u001b[0m     left_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m custom_loss: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_train(loader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, custom_loss),\n\u001b[1;32m     84\u001b[0m     right_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m sup_data: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_train(loader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, sup_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_cache()\n",
         "File \u001b[0;32m~/.local/share/virtualenvs/ptls-experiments-H-SwwRmK/lib/python3.8/site-packages/pymonad/either.py:91\u001b[0m, in \u001b[0;36mEither.either\u001b[0;34m(self, left_function, right_function)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m right_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mleft_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonoid\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/models/network_impl/base_nn_model.py:81\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(custom_loss)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 81\u001b[0m fit_output \u001b[38;5;241m=\u001b[39m Either(value\u001b[38;5;241m=\u001b[39msupplementary_data,\n\u001b[1;32m     82\u001b[0m                     monoid\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_loss, custom_fit_process])\u001b[38;5;241m.\u001b[39meither(\n\u001b[1;32m     83\u001b[0m     left_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m custom_loss: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_train(loader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, custom_loss),\n\u001b[1;32m     84\u001b[0m     right_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m sup_data: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_train(loader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, sup_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_cache()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/models/network_impl/base_nn_model.py:139\u001b[0m, in \u001b[0;36m_default_train\u001b[0;34m(self, train_loader, model, custom_loss)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_train\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m                    train_loader,\n\u001b[1;32m    137\u001b[0m                    model,\n\u001b[1;32m    138\u001b[0m                    custom_loss: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    141\u001b[0m         model_loss, avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_loop(train_loader, model, custom_loss)\n",
         "File \u001b[0;32m/ptls-experiments/FedCore/fedcore/models/network_impl/base_nn_model.py:96\u001b[0m, in \u001b[0;36m_train_loop\u001b[0;34m(self, train_loader, model, custom_loss)\u001b[0m\n\u001b[1;32m     94\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     97\u001b[0m     total_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     98\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m batch\n",
         "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable PaddedBatch object"
        ]
       }
      ],
      "source": [
       "from ptls.frames.bert.losses.query_soft_max import QuerySoftmaxLoss\n",
       "\n",
       "run_experiment('compression_experiments/composite_age/mlm', \n",
       "               mlm_dm,\n",
       "               partial(QuerySoftmaxLoss, temperature=20.0, reduce=False),\n",
       "               define_mlm,\n",
       "               fedcore_setup = {'compression_task': 'composite_compression',\n",
       "                    'model_params': dict(pruning_model=dict(epochs=5,\n",
       "                                                            pruning_iterations=3,\n",
       "                                                            learning_rate=0.001,\n",
       "                                                            importance='MagnitudeImportance',\n",
       "                                                            pruner_name='magnitude_pruner',\n",
       "                                                            importance_norm=1,\n",
       "                                                            pruning_ratio=0.75,\n",
       "                                                            finetune_params={'epochs': 5,\n",
       "                                                                             'custom_loss': None}\n",
       "                                                            ),\n",
       "                                         low_rank_model=dict(epochs=30,\n",
       "                                                             learning_rate=0.001,\n",
       "                                                             hoyer_loss=0.2,\n",
       "                                                             energy_thresholds=[0.9],\n",
       "                                                             orthogonal_loss=5,\n",
       "                                                             decomposing_mode='channel',\n",
       "                                                             spectrum_pruning_strategy='energy',\n",
       "                                                             finetune_params={'epochs': 10,\n",
       "                                                                              'custom_loss': None}\n",
       "                                                             ),\n",
       "                                         training_model=dict(\n",
       "                                             epochs=100,\n",
       "                                         )                    \n",
       "                                         ),\n",
       "                                         \n",
       "                    'initial_assumption': ['training_model',\n",
       "                                            # 'pruning_model', \n",
       "                                            # 'low_rank_model',\n",
       "                                            #   'pruning_model'\n",
       "                                              ]},\n",
       "              n =1\n",
       "              )"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# GPT"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "2024-09-23 14:35:37,041 - Loaded 1332 records\n"
        ]
       },
       {
        "ename": "TypeError",
        "evalue": "__init__() missing 2 required positional arguments: 'min_len' and 'max_len'",
        "output_type": "error",
        "traceback": [
         "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
         "Cell \u001b[0;32mIn[108], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mptls\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GptPretrainModule\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mptls\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GptDataset\n\u001b[1;32m      4\u001b[0m ptls_data_module_gpt \u001b[38;5;241m=\u001b[39m PtlsDataModule(\n\u001b[0;32m----> 5\u001b[0m     train_data\u001b[38;5;241m=\u001b[39m\u001b[43mGptDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMemoryMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mi_filters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mSeqLenFilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSampleSlices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43msplit_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcnt_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcnt_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     18\u001b[0m     train_num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     19\u001b[0m     train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     20\u001b[0m     test_data\u001b[38;5;241m=\u001b[39mGptDataset(\n\u001b[1;32m     21\u001b[0m         MemoryMapDataset(\n\u001b[1;32m     22\u001b[0m             data\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m     23\u001b[0m             i_filters\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     24\u001b[0m                 SeqLenFilter(min_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m),\n\u001b[1;32m     25\u001b[0m             ],\n\u001b[1;32m     26\u001b[0m         ),\n\u001b[1;32m     27\u001b[0m         splitter\u001b[38;5;241m=\u001b[39mSampleSlices(\n\u001b[1;32m     28\u001b[0m             split_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     29\u001b[0m             cnt_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,\n\u001b[1;32m     30\u001b[0m             cnt_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     31\u001b[0m         ),\n\u001b[1;32m     32\u001b[0m     ),\n\u001b[1;32m     33\u001b[0m     test_num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     34\u001b[0m     test_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     35\u001b[0m )\n",
         "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'min_len' and 'max_len'"
        ]
       }
      ],
      "source": [
       "from ptls.frames.gpt.gpt_module import GptPretrainModule\n",
       "from ptls.frames.gpt.gpt_dataset import GptDataset\n",
       "\n",
       "ptls_data_module_gpt = PtlsDataModule(\n",
       "    train_data=GptDataset(\n",
       "        MemoryMapDataset(\n",
       "            data=train,\n",
       "            i_filters=[\n",
       "                SeqLenFilter(min_seq_len=25),\n",
       "            ],\n",
       "        ),\n",
       "        splitter=SampleSlices(\n",
       "            split_count=5,\n",
       "            cnt_min=25,\n",
       "            cnt_max=200,\n",
       "        ),\n",
       "    ),\n",
       "    train_num_workers=4,\n",
       "    train_batch_size=256,\n",
       "    test_data=GptDataset(\n",
       "        MemoryMapDataset(\n",
       "            data=test,\n",
       "            i_filters=[\n",
       "                SeqLenFilter(min_seq_len=25),\n",
       "            ],\n",
       "        ),\n",
       "        splitter=SampleSlices(\n",
       "            split_count=5,\n",
       "            cnt_min=25,\n",
       "            cnt_max=200,\n",
       "        ),\n",
       "    ),\n",
       "    test_num_workers=4,\n",
       "    test_batch_size=256,\n",
       ")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def define_coles():\n",
       "    trx_encoder_params = dict(\n",
       "    embeddings_noise=0.003,\n",
       "    numeric_values={'amount_rur': 'identity'},\n",
       "    embeddings={\n",
       "        'trans_date': {'in': 800, 'out': 16},\n",
       "        'small_group': {'in': 250, 'out': 16},\n",
       "    },\n",
       ")\n",
       "\n",
       "    seq_encoder = RnnSeqEncoder(\n",
       "        trx_encoder=TrxEncoder(**trx_encoder_params),\n",
       "        hidden_size=256,\n",
       "        type='gru',\n",
       "    )\n",
       "\n",
       "    model = GptPretrainModule(\n",
       "        seq_encoder=seq_encoder,\n",
       "        optimizer_partial=partial(torch.optim.Adam, lr=0.001),\n",
       "        lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=30, gamma=0.9),\n",
       "        head=Head(use_norm_encoder=True, \n",
       "                input_size=256,\n",
       "                hidden_layers_sizes=[256, 256])\n",
       "    )\n",
       "    return model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "2024-09-23 11:14:06,070 - Initialising Industrial Repository\n",
         "2024-09-23 11:14:06,365 - Initialising solver\n",
         "2024-09-23 11:14:06,366 - Initialising experiment setup\n",
         "Forcely substituted loss to ContrastiveLoss()\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:26<00:00,  1.35it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 1, Average loss 1477.1728312439388\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 9/9 [00:04<00:00,  2.08it/s]"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "==============Prepare original model for pruning=================\n",
         "==============Initialisation of pruning agent=================\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Forcely substituted loss to ContrastiveLoss()\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:26<00:00,  1.37it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 1, Average loss 136.5059274037679\n",
         "==============Finetune pruned model=================\n",
         "Forcely substituted loss to ContrastiveLoss()\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:26<00:00,  1.34it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 1, Average loss 124.39279308584001\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:27<00:00,  1.32it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 2, Average loss 119.3623763985104\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:27<00:00,  1.31it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 3, Average loss 114.56469586160448\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:27<00:00,  1.33it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 4, Average loss 110.45625359482236\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:27<00:00,  1.31it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 5, Average loss 110.38965015941196\n",
         "==============After pruning=================\n",
         "Params: 0.37 M => 0.37 M\n",
         "MACs: 0.00 G => 0.00 G\n",
         "Forcely substituted loss to ContrastiveLoss()\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:23<00:00,  1.56it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 1, Average loss 107.02375711335077, orthogonal_loss: 9.167923, hoer_loss: 2.624759, metric_loss: 6.543130\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:23<00:00,  1.55it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 2, Average loss 102.59809509913127, orthogonal_loss: 8.491811, hoer_loss: 2.529775, metric_loss: 5.962034\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:23<00:00,  1.53it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 3, Average loss 99.99738940927718, orthogonal_loss: 9.540761, hoer_loss: 2.430923, metric_loss: 7.109838\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:23<00:00,  1.54it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 4, Average loss 98.73699593544006, orthogonal_loss: 8.471926, hoer_loss: 2.330903, metric_loss: 6.141023\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:24<00:00,  1.50it/s]"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 5, Average loss 97.28895076115926, orthogonal_loss: 7.671278, hoer_loss: 2.232582, metric_loss: 5.438696\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "==============Truncate rank for each weight matrix=================\n",
         "==============Finetune truncated model=================\n",
         "Forcely substituted loss to ContrastiveLoss()\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:23<00:00,  1.55it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 1, Average loss 97.20061695575714\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:23<00:00,  1.56it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 2, Average loss 94.50033469994862\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:24<00:00,  1.48it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 3, Average loss 94.65465924474928\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:23<00:00,  1.55it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 4, Average loss 93.06651304827795\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:23<00:00,  1.53it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 5, Average loss 92.19936703311072\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:24<00:00,  1.49it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 6, Average loss 90.97706390751733\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:23<00:00,  1.54it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 7, Average loss 92.9701257944107\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:23<00:00,  1.51it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 8, Average loss 92.77966568205092\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:24<00:00,  1.47it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 9, Average loss 91.38662118381924\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "100%|██████████| 36/36 [00:24<00:00,  1.50it/s]\n"
        ]
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Epoch: 10, Average loss 90.65867667728\n",
         "==============After low rank truncation=================\n",
         "Params: 0.50 M => 0.50 M\n",
         "MACs: 0.00 G => 0.00 G\n"
        ]
       }
      ],
      "source": [
       "fedcore_compressor.fit((input_data, model), manually_done=True)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/plain": [
          "ContrastiveLoss()"
         ]
        },
        "execution_count": 17,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "from functools import partial\n",
       "from ptls.frames.coles.sampling_strategies.hard_negative_pair_selector import HardNegativePairSelector\n",
       "from fedcore.losses.ptls_losses import ContrastiveLoss, VicregLoss\n",
       "\n",
       "partial(ContrastiveLoss, margin=0.5, sampling_strategy=HardNegativePairSelector(neg_count=5))()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
       {
        "ename": "NameError",
        "evalue": "name 'compression_pipeline' is not defined",
        "output_type": "error",
        "traceback": [
         "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
         "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
         "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m quant_model \u001b[38;5;241m=\u001b[39m \u001b[43mcompression_pipeline\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(input_data)\u001b[38;5;241m.\u001b[39mpredict\n\u001b[0;32m      2\u001b[0m quant_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
         "\u001b[1;31mNameError\u001b[0m: name 'compression_pipeline' is not defined"
        ]
       }
      ],
      "source": [
       "quant_model = compression_pipeline.predict(input_data).predict\n",
       "quant_model.save('./output')"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "evaluator = PerformanceEvaluator(model, dataset, batch_size=64)\n",
       "performance = evaluator.eval()\n",
       "print('after quantization')\n",
       "print(performance)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "from ptls.frames.coles.coles_module import CoLESModule\n",
       "from ptls.frames.cpc.modules.cpc_module import CpcModule\n",
       "from ptls.frames.bert.modules.mlm_module import MLMPretrainModule\n",
       "from ptls.frames.gpt.gpt_module\n",
       "from ptls.frames.tabformer.tabformer_module import "
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# EVALUATION"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "dir_ = 'compression_experiments/composite_age/coles'\n",
       "for m in os.listdir(dir_):\n",
       "    try:\n",
       "        path = os.path.join(dir_, m, 'model_0_or.pkl')\n",
       "        model_ = load_pkl(path)\n",
       "        evaluator = R.PerformanceEvaluator(model_, ptls_data_module.test_dataloader(), batch_size=64)\n",
       "        performance = evaluator.eval()\n",
       "        print(m)\n",
       "        print(performance)\n",
       "    except:\n",
       "        continue\n",
       "    \n",
       "\n",
       "from fedcore.tools.ruler import PerformanceEvaluator\n",
       "\n",
       "\n",
       "evaluator = PerformanceEvaluator(model, ptls_data_module.test_dataloader(), batch_size=64)\n",
       "performance = evaluator.eval()\n",
       "print('after quantization')\n",
       "print(performance)"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }
   