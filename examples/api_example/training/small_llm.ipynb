{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e3bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Правильный путь с учетом вложенности\n",
    "correct_path = \"/home/user/projects/FedCore/FedCore\"\n",
    "sys.path.insert(0, correct_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3075dffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/projects/FedCore/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"arnir0/Tiny-LLM\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,  use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bde36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a41f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/projects/FedCore/.venv/lib/python3.10/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompressionInputData parameters:\n",
      "  self: <class 'inspect._empty'> = <class 'inspect._empty'>\n",
      "  features: <class 'numpy.ndarray'> = [[0. 0.]\n",
      " [0. 0.]]\n",
      "  target: typing.Optional[numpy.ndarray] = None\n",
      "  train_dataloader: <class 'torch.utils.data.dataloader.DataLoader'> = None\n",
      "  val_dataloader: <class 'torch.utils.data.dataloader.DataLoader'> = None\n",
      "  test_dataloader: <class 'torch.utils.data.dataloader.DataLoader'> = None\n",
      "  task: <class 'fedot.core.repository.tasks.Task'> = Task(task_type=<TaskTypesEnum.classification: 'classification'>, task_params=None)\n",
      "  num_classes: <class 'int'> = None\n",
      "  input_dim: <class 'int'> = None\n",
      "  supplementary_data: <class 'fedot.core.data.supplementary_data.SupplementaryData'> = <factory>\n"
     ]
    }
   ],
   "source": [
    "from fedcore.data.data import CompressionInputData\n",
    "import inspect\n",
    "print(\"CompressionInputData parameters:\")\n",
    "sig = inspect.signature(CompressionInputData.__init__)\n",
    "for param_name, param in sig.parameters.items():\n",
    "    print(f\"  {param_name}: {param.annotation} = {param.default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b238178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "# from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd80e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем модель из Hugging Face Hub...\n",
      "Сохраняем модель в локальную директорию: ./tiny-llm-model\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_MODEL = \"./tiny-llm-model\"\n",
    "if os.path.exists(PATH_TO_MODEL) and os.listdir(PATH_TO_MODEL):\n",
    "    print(f\"Загружаем модель из локальной директории: {PATH_TO_MODEL}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PATH_TO_MODEL, use_fast=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(PATH_TO_MODEL)\n",
    "else:\n",
    "    print(\"Загружаем модель из Hugging Face Hub...\")\n",
    "    MODEL_NAME = \"arnir0/Tiny-LLM\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    print(f\"Сохраняем модель в локальную директорию: {PATH_TO_MODEL}\")\n",
    "    tokenizer.save_pretrained(PATH_TO_MODEL)\n",
    "    model.save_pretrained(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f13dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'arnir0/Tiny-LLM'\n",
    "dataset_name = \"imdb\"\n",
    "# output_dir = \"./qwen2-0.5b-imdb-finetuned\"\n",
    "output_dir = './tiny-llm'\n",
    "max_length = 512  # Maximum context length for each sample\n",
    "\n",
    "# Use 4-bit quantization to drastically reduce memory usage\n",
    "use_4bit = False\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "\n",
    "# LoRA configuration for Parameter-Efficient Fine-Tuning\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# Training arguments\n",
    "num_train_epochs = 3\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "learning_rate = 2e-4\n",
    "logging_steps = 10\n",
    "save_steps = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a34452",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)\n",
    "# Set padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with 4-bit quantization if enabled\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "if use_4bit:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",  # Automatically places layers on available GPUs\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cpu\", #auto\n",
    "        torch_dtype=torch.float32,\n",
    "        trust_remote_code=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbf5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1000\n",
      "Dataset features: {'text': Value('string')}\n",
      "Training samples: 517\n",
      "Validation samples: 130\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1000]\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Dataset features: {dataset.features}\")\n",
    "\n",
    "def preprocess_dataset(examples):\n",
    "    \"\"\"Extract text from different dataset formats\"\"\"\n",
    "    if 'text' in examples:\n",
    "        return {\"text\": examples[\"text\"]}\n",
    "    elif 'article' in examples:  # CNN Daily Mail\n",
    "        return {\"text\": examples[\"article\"]}\n",
    "    elif 'content' in examples:  # Some datasets\n",
    "        return {\"text\": examples[\"content\"]}\n",
    "    elif 'sentence' in examples:  # Some sentence datasets\n",
    "        return {\"text\": examples[\"sentence\"]}\n",
    "    else:\n",
    "        # Try to use the first string column\n",
    "        for key, value in examples.items():\n",
    "            if isinstance(value[0], str):\n",
    "                return {\"text\": examples[key]}\n",
    "        return {\"text\": [str(x) for x in examples[list(examples.keys())[0]]]}\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset = dataset.map(preprocess_dataset, batched=True)\n",
    "\n",
    "# Filter out empty texts\n",
    "dataset = dataset.filter(lambda example: len(example[\"text\"].strip()) > 0)\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,  # Reduced for tiny model\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split dataset\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "025895f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "219b8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We are doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Load accuracy metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Shift labels and predictions for causal LM (next token prediction)\n",
    "    # Predictions are for the next token, so we shift labels accordingly\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    # Flatten the tokens and get predictions\n",
    "    predictions = np.argmax(shift_logits, axis=-1).flatten()\n",
    "    labels = shift_labels.flatten()\n",
    "    \n",
    "    # Calculate accuracy, ignoring padding tokens (where label = -100)\n",
    "    mask = labels != -100\n",
    "    predictions = predictions[mask]\n",
    "    labels = labels[mask]\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd990f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    # logging_steps=logging_steps,\n",
    "    logging_steps=5,\n",
    "    save_steps=save_steps,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,  # Disable external logging like Weights & Biases for simplicity\n",
    "    fp16=False,  # Use mixed precision training\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
