2025-10-27 17:07:47,904 - __main__ - INFO - Loading model from HuggingFace: arnir0/Tiny-LLM
2025-10-27 17:07:48,378 - __main__ - INFO - Model loaded: LlamaForCausalLM
2025-10-27 17:07:48,691 - __main__ - INFO - Tokenizer loaded. Vocabulary size: 32000
2025-10-27 17:07:48,691 - __main__ - INFO - Generated fedcore_id: fedcore_f2d0fb9b
2025-10-27 17:07:54,108 - __main__ - INFO - Dataset loaded: 1000 samples
2025-10-27 17:07:54,114 - __main__ - INFO - Dataset filtered: 647 samples
2025-10-27 17:07:54,130 - __main__ - INFO - Datasets split: train=517, val=64, test=66
2025-10-27 17:07:54,136 - __main__ - INFO - Example batch shape: torch.Size([4, 64])
2025-10-27 17:08:02,021 - __main__ - INFO - Using fedcore_id: None
2025-10-27 17:08:02,694 - __main__ - INFO - Memory after cleanup: 0.1147 GB
2025-10-27 17:08:02,694 - __main__ - INFO - FINAL STATISTICS
2025-10-27 17:08:02,695 - __main__ - INFO - Training time:     6.14 sec
2025-10-27 17:08:02,695 - __main__ - INFO - Report time:       1.74 sec
2025-10-27 17:08:02,695 - __main__ - INFO - Total time:        14.12 sec
2025-10-27 17:08:02,695 - __main__ - INFO - MEMORY STATISTICS:
2025-10-27 17:08:02,695 - __main__ - INFO - Initial GPU memory:     0.0000 GB
2025-10-27 17:08:02,695 - __main__ - INFO - After training:          0.1642 GB
2025-10-27 17:08:02,695 - __main__ - INFO - After report:            0.1642 GB
2025-10-27 17:08:02,695 - __main__ - INFO - Final GPU memory:        0.1147 GB
2025-10-27 17:08:02,695 - __main__ - INFO - Peak memory:             0.1642 GB
2025-10-27 17:08:02,695 - __main__ - INFO - Memory freed:             0.0495 GB
2025-10-27 17:08:02,695 - __main__ - INFO - Cleanup efficiency:      30.1%
